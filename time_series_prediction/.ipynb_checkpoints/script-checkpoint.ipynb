{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math, keras, datetime, pandas as pd, numpy as np, keras.backend as K\n",
    "import matplotlib.pyplot as plt, xgboost, operator, random, pickle\n",
    "import glob, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Introduction\n",
    "# This is an initial Exploratory Data Analysis for the Recruit Restaurant Visitor Forecasting \n",
    "\n",
    "# The aim of this challenge is to predict the future numbers of restaurant visitors. \n",
    "# This makes it a Time Series Forecasting problem. The data was collected from Japanese restaurants.\n",
    "# As we will see, the data set is small and easily accessible without requiring much memory or computing power. \n",
    "# Therefore, this competition is particularly suited for beginners.\n",
    "\n",
    "# The data comes in the shape of 8 relational files which are derived from two separate Japanese websites that \n",
    "# collect user information: “Hot Pepper Gourmet (hpg): similar to Yelp” (search and reserve) \n",
    "# and “AirREGI / Restaurant Board (air): similar to Square” (reservation control and cash register). \n",
    "# The training data is based on the time range of Jan 2016 - most of Apr 2017, \n",
    "# while the test set includes the last week of Apr plus May 2017. \n",
    "# The test data “intentionally spans a holiday week in Japan called the ‘Golden Week.’ \n",
    "# The data description further notes that:”There are days in the test set where the restaurant were \n",
    "# closed and had no visitors. These are ignored in scoring. The training set omits days where the restaurants \n",
    "# were closed.\"\n",
    "\n",
    "# Those are the individual files:\n",
    "\n",
    "# air_visit_data.csv: historical visit data for the air restaurants. \n",
    "# This is essentially the main training data set.\n",
    "\n",
    "# air_reserve.csv / hpg_reserve.csv: reservations made through the air / hpg systems.\n",
    "\n",
    "# air_store_info.csv / hpg_store_info.csv: details about the air / hpg restaurants including genre and location.\n",
    "\n",
    "# store_id_relation.csv: connects the air and hpg ids\n",
    "\n",
    "# date_info.csv: essentially flags the Japanese holidays.\n",
    "\n",
    "# sample_submission.csv: serves as the test set. The id is formed by combining the air id with the visit date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_visits = pd.read_csv('/Users/midas/pcc/input/air_visit_data.csv')\n",
    "air_reserve = pd.read_csv('/Users/midas/pcc/input/air_reserve.csv')\n",
    "hpg_reserve = pd.read_csv('/Users/midas/pcc/input/hpg_reserve.csv')\n",
    "air_store = pd.read_csv('/Users/midas/pcc/input/air_store_info.csv')\n",
    "hpg_store = pd.read_csv('/Users/midas/pcc/input/hpg_store_info.csv')\n",
    "holidays = pd.read_csv('/Users/midas/pcc/input/date_info.csv')\n",
    "store_ids = pd.read_csv('/Users/midas/pcc/input/store_id_relation.csv')\n",
    "test = pd.read_csv('/Users/midas/pcc/input/sample_submission.csv')\n",
    "# test_df = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [air_visits, air_reserve, hpg_reserve, air_store, \n",
    "               hpg_store, holidays, store_ids, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tables: display(t.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to combine the hgc data to the air data?? \n",
    "air_store_id\tvisit_date\tvisitors \t air_genre_name\t\t air_area_name\tlatitude\tlongitude\n",
    "day_of_week\t holiday_flg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in tables: display(DataFrameSummary(t).summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables = [air_visits, air_reserve, hpg_reserve, air_store, \n",
    "#                hpg_store, holidays, store_ids, test]\n",
    "\n",
    "# air_visits, air_store, holidays easy to join\n",
    "# air reserve < air_visits\n",
    "# store_ids !== total air stores\n",
    "# hpg_store > air_store\n",
    "# hpg_reserve > air_visits\n",
    "\n",
    "test needs to get added with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(air_visits), len(test), len(store_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.holiday_flg = holidays.holiday_flg != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_df(left, right, left_on, right_on=None):\n",
    "    if right_on is None: right_on = left_on\n",
    "    return left.merge(right, how='left', left_on=left_on, right_on=right_on, \n",
    "                      suffixes=(\"\", \"_y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you sort of remember this but you create a inner join between two files based on identical identifier\n",
    "# in the case of holidays you can join based on the visit_date and calender date.\n",
    "\n",
    "training = join_df(air_visits, holidays, \"visit_date\", \"calendar_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = join_df(test, holidays, \"visit_date\", \"calendar_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# air_visits = join_df(air_visits, air_reserve, \"air_store_id\", \"air_store_id\")\n",
    "# num of unique air_reserve ids !== num of unique air_visit ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training2 = join_df(training, air_store, \"air_store_id\", \"air_store_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training[training.holiday_flg.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(training2[training2.air_genre_name.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils2 import *\n",
    "np.set_printoptions(threshold=50, edgeitems=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isoweek import Week\n",
    "from pandas_summary import DataFrameSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_csvs(dirname):\n",
    "    os.chdir(dirname)\n",
    "    filenames=glob.glob(\"*.csv\")\n",
    "\n",
    "    wrote_header = False\n",
    "    with open(\"../\"+dirname+\".csv\",\"w\") as outputfile:\n",
    "        for filename in filenames:\n",
    "            name = filename.split(\".\")[0]\n",
    "            with open(filename) as f:\n",
    "                line = f.readline()\n",
    "                if not wrote_header:\n",
    "                    wrote_header = True\n",
    "                    outputfile.write(\"file,\"+line)\n",
    "                for line in f:\n",
    "                     outputfile.write(name + \",\" + line)\n",
    "                outputfile.write(\"\\n\")\n",
    "\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_csvs('input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/midas/pcc/input/air_visit_data.csv')\n",
    "df = pd.read_csv('/Users/midas/pcc/input/air_store_info.csv')\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by id and day of week - Median of the visitors is taken\n",
    "agg_data = train.groupby(['air_store_id', 'dow']).agg(aggregation).reset_index()\n",
    "agg_data.columns = ['air_store_id','dow','visitors']\n",
    "agg_data['visitors'] = agg_data['visitors']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the first intermediate submission file:\n",
    "merged = pd.merge(test_df, agg_data, how='left', left_on=[\n",
    "    'store_id','dow'], right_on=['air_store_id','dow'])\n",
    "final = merged[['id','visitors']]\n",
    "final.fillna(0, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally from this kernel:\n",
    "# https://www.kaggle.com/zeemeen/weighted-mean-running-10-sec-lb-0-509\n",
    "dfs = { re.search('/([^/\\.]*)\\.csv', fn).group(1):pd.read_csv(\n",
    "    fn) for fn in glob.glob('../input/*.csv')}\n",
    "for k, v in dfs.items(): locals()[k] = v\n",
    "\n",
    "weekend_hdays = date_info.apply(\n",
    "    (lambda x:(x.day_of_week=='Sunday' or x.day_of_week=='Saturday') \n",
    "    and x.holiday_flg==1), axis=1)\n",
    "date_info.loc[weekend_hdays, 'holiday_flg'] = 0\n",
    "date_info['weight'] = (date_info.index + 1) / len(date_info) \n",
    "\n",
    "visit_data = air_visit_data.merge(\n",
    "    date_info, left_on='visit_date', right_on='calendar_date', how='left')\n",
    "visit_data.drop('calendar_date', axis=1, inplace=True)\n",
    "visit_data['visitors'] = visit_data.visitors.map(pd.np.log1p)\n",
    "\n",
    "wmean = lambda x:( (x.weight * x.visitors).sum() / x.weight.sum() )\n",
    "visitors = visit_data.groupby(\n",
    "    ['air_store_id', 'day_of_week', 'holiday_flg']).apply(wmean).reset_index()\n",
    "visitors.rename(columns={0:'visitors'}, inplace=True) \n",
    "\n",
    "sample_submission['air_store_id'] = sample_submission.id.map(\n",
    "    lambda x: '_'.join(x.split('_')[:-1]))\n",
    "sample_submission['calendar_date'] = sample_submission.id.map(lambda x: x.split('_')[2])\n",
    "sample_submission.drop('visitors', axis=1, inplace=True)\n",
    "sample_submission = sample_submission.merge(date_info, on='calendar_date', how='left')\n",
    "sample_submission = sample_submission.merge(\n",
    "    visitors, on=['air_store_id', 'day_of_week', 'holiday_flg'], how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missings with (air_store_id, day_of_week)\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[visitors.holiday_flg==0], on=(\n",
    "        'air_store_id', 'day_of_week'), how='left')['visitors_y'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missings with (air_store_id)\n",
    "missings = sample_submission.visitors.isnull()\n",
    "sample_submission.loc[missings, 'visitors'] = sample_submission[missings].merge(\n",
    "    visitors[['air_store_id', 'visitors']].groupby('air_store_id').mean().reset_index(), \n",
    "    on='air_store_id', how='left')['visitors_y'].values\n",
    "    \n",
    "sample_submission['visitors'] = sample_submission.visitors.map(pd.np.expm1)\n",
    "sample_submission = sample_submission[['id', 'visitors']]\n",
    "final['visitors'][final['visitors'] ==0] = sample_submission['visitors'][final['visitors'] ==0]\n",
    "sub_file = final.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Arithmetric Mean \n",
    "sub_file['visitors'] = np.mean([final['visitors'], sample_submission['visitors']], axis = 0)\n",
    "sub_file.to_csv('sub_math_mean.csv', index=False)\n",
    "\n",
    "## Geometric Mean  \n",
    "sub_file['visitors'] = (final['visitors'] * sample_submission['visitors']) ** (1/2)\n",
    "sub_file.to_csv('sub_geo_mean.csv', index=False)\n",
    "\n",
    "## Harmonic Mean \n",
    "sub_file['visitors'] = 2/(1/final['visitors'] + 1/sample_submission['visitors'])\n",
    "sub_file.to_csv('sub_hrm_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
